<?xml version="1.0"?>
<rss version="2.0">
  <channel>
    <title>Reproducibility News Feed</title>
    <link>http://reproduciblescience.org/</link>
    <description>A feed that shows recent news about scientific reproducibility efforts.</description>

    <item>
      <title>The RAMP framework: from reproducibility to transparency in the design and optimization of scientific workflows</title>
      <link>https://openreview.net/pdf?id=Syg4NHz4eQ</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        The RAMP (Rapid Analytics and Model Prototyping) is a software and project management tool developed by the Paris-Saclay Center for Data Science. The original goal was to accelerate the adoption of high-quality data science solutions for domain science problems by running rapid collaborative prototyping sessions. Today it is a full-blown data science project management tool promoting reproducibility, fair and transparent model evaluation, and democratization of data science.  We have used the framework for setting up and solving about twenty scientific problems, for organizing scientific sub-communities around these events, and for training novice data scientists.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Three Dimensions of Reproducibility in Natural Language Processing</title>
      <link>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5998676/</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        Despite considerable recent attention to problems with reproducibility of scientific research, there is a striking lack of agreement about the definition of the term. That is a problem, because the lack of a consensus definition makes it difficult to compare studies of reproducibility, and thus to have even a broad overview of the state of the issue in natural language processing. This paper proposes an ontology of reproducibility in that field. Its goal is to enhance both future research and communication about the topic, and retrospective meta-analyses. We show that three dimensions of reproducibility, corresponding to three kinds of claims in natural language processing papers, can account for a variety of types of research reports. These dimensions are reproducibility of a conclusion, of a finding, and of a value. Three biomedical natural language processing papers by the authors of this paper are analyzed with respect to these dimensions.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Enabling the Verification of Computational Results: An Empirical Evaluation of Computational Reproducibility</title>
      <link>https://dl.acm.org/citation.cfm?doid=3214239.3214242</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        The ability to independently regenerate published computational claims is widely recognized as a key component of scientific reproducibility. In this article we take a narrow interpretation of this goal, and attempt to regenerate published claims from author-supplied information, including data, code, inputs, and other provided specifications, on a different computational system than that used by the original authors. We are motivated by Claerbout and Donoho's exhortation of the importance of providing complete information for reproducibility of the published claim. We chose the Elsevier journal, the Journal of Computational Physics, which has stated author guidelines that encourage the availability of computational digital artifacts that support scholarly findings. In an IRB approved study at the University of Illinois at Urbana-Champaign (IRB #17329) we gathered artifacts from a sample of authors who published in this journal in 2016 and 2017. We then used the ICERM criteria generated at the 2012 ICERM workshop "Reproducibility in Computational and Experimental Mathematics" to evaluate the sufficiency of the information provided in the publications and the ease with which the digital artifacts afforded computational reproducibility. We find that, for the articles for which we obtained computational artifacts, we could not easily regenerate the findings for 67% of them, and we were unable to easily regenerate all the findings for any of the articles. We then evaluated the artifacts we did obtain (55 of 306 articles) and find that the main barriers to computational reproducibility are inadequate documentation of code, data, and workflow information (70.9%), missing code function and setting information, and missing licensing information (75%). We recommend improvements based on these findings, including the deposit of supporting digital artifacts for reproducibility as a condition of publication, and verification of computational findings via re-execution of the code when possible.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Improving Reproducibility of Distributed Computational Experiments</title>
      <link>https://dl.acm.org/citation.cfm?doid=3214239.3214241</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        Conference and journal publications increasingly require experiments associated with a submitted article to be repeatable. Authors comply to this requirement by sharing all associated digital artifacts, i.e., code, data, and environment configuration scripts. To ease aggregation of the digital artifacts, several tools have recently emerged that automate the aggregation of digital artifacts by auditing an experiment execution and building a portable container of code, data, and environment. However, current tools only package non-distributed computational experiments. Distributed computational experiments must either be packaged manually or supplemented with sufficient documentation. In this paper, we outline the reproducibility requirements of distributed experiments using a distributed computational science experiment involving use of message-passing interface (MPI), and propose a general method for auditing and repeating distributed experiments. Using Sciunit we show how this method can be implemented. We validate our method with initial experiments showing application re-execution runtime can be improved by 63% with a trade-off of longer run-time on initial audit execution.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Popper Pitfalls</title>
      <link>https://dl.acm.org/citation.cfm?doid=3214239.3214243</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        We describe the four publications we have tried to make reproducible and discuss how each paper has changed our workflows, practices, and collaboration policies. The fundamental insight is that paper artifacts must be made reproducible from the start of the project; artifacts are too difficult to make reproducible when the papers are (1) already published and (2) authored by researchers that are not thinking about reproducibility. In this paper, we present the best practices adopted by our research laboratory, which was sculpted by the pitfalls we have identified for the Popper convention. We conclude with a "call-to-arms" for the community focused on enhancing reproducibility initiatives for academic conferences, industry environments, and national laboratories. We hope that our experiences will shape a best practices guide for future reproducible papers.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Supporting Long-term Reproducible Software Execution</title>
      <link>https://dl.acm.org/citation.cfm?doid=3214239.3214245</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        A recent widespread realization that software experiments are not as easily replicated as once believed brought software execution preservation to the science spotlight. As a result, scientists, institutions, and funding agencies have recently been pushing for the development of methodologies and tools that preserve software artifacts. Despite current efforts, long term reproducibility still eludes us. In this paper, we present the requirements for software execution preservation and discuss how to improve long-term reproducibility in science. In particular, we discuss the reasons why preserving binaries and pre-built execution environments is not enough and why preserving the ability to replicate results is not the same as preserving software for reproducible science. Finally, we show how these requirements are supported by Occam, an open curation framework that fully preserves software and its dependencies from source to execution, promoting transparency, longevity, and re-use. Specifically, Occam provides the ability to automatically deploy workflows in a fully-functional environment that is able to not only run them, but make them easily replicable.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Have researchers increased reporting of outliers in response to the reproducibility crisis?</title>
      <link>https://osf.io/jq8eu</link>
      <pubDate>Sat, 02 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        Psychology is currently experiencing a "renaissance" where the replication and reproducibility of published reports are at the forefront of conversations in the field. While researchers have worked to discuss possible problems and solutions, work has yet to uncover how this new culture may have altered reporting practices in the social sciences. As outliers can bias both descriptive and inferential statistics, the search for these data points is essential to any analysis using these parameters. We quantified the rates of reporting of outliers within psychology at two time points: 2012 when the replication crisis was born, and 2017, after the publication of reports concerning replication, questionable research practices, and transparency. A total of 2235 experiments were identified and analyzed, finding an increase in reporting of outliers from only 15.7% of experiments mentioning outliers in 2012 to 25.0% in 2017. We investigated differences across years given the psychological field or statistical analysis that experiment employed. Further, we inspected whether outliers mentioned are whole participant observations or data points, and what reasons authors gave for stating the observation was deviant. We conclude that while report rates are improving overall, there is still room for improvement in the reporting practices of psychological scientists which can only aid in strengthening our science.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Sharing and Preserving Computational Analyses for Posterity with encapsulator</title>
      <link>http://tfjmp.org/files/publications/cise-2018.pdf</link>
      <pubDate>Sat, 02 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        Open  data  and  open-source  software  may be  part  of  the  solution  to  sciences  reproducibility crisis,   but   they   are   insufficient   to   guarantee   reproducibility. Requiring minimal end-user expertise, encapsulator creates a "time capsule" with reproducible code (right now, only supporting R code) in a self-contained computational environment. encapsulator provides end-users with a fully-featured desktop environment for reproducible research.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>The Crisis of Reproducibility, the Denominator Problem and the Scientific Role of Multi-Scale Modeling</title>
      <link>https://www.preprints.org/manuscript/201805.0308/v2</link>
      <pubDate>Tue, 29 May 2018 00:00:00 -0000</pubDate>
      <description>
        The "Crisis of Reproducibility" has received considerable attention both within the scientific community and without. While factors associated with scientific culture and practical practice are most often invoked, I propose that the Crisis of Reproducibility is ultimately a failure of generalization with a fundamental scientific basis in the methods used for biomedical research. The Denominator Problem describes how limitations intrinsic to the two primary approaches of biomedical research, clinical studies and pre-clinical experimental biology, lead to an inability to effectively characterize the full extent of biological heterogeneity, which compromises the task of generalizing acquired knowledge. Drawing on the example of the unifying role of theory in the physical sciences, I propose that multi-scale mathematical and dynamic computational models, when mapped to the modular structure of biological systems, can serve a unifying role as formal representations of what is conserved and similar from one biological context to another. This ability to explicitly describe the generation of heterogeneity from similarity addresses the Denominator Problem and provides a scientific response to the Crisis of Reproducibility.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>The 'end of the expert': why science needs to be above criticism</title>
      <link>https://www.repository.cam.ac.uk/handle/1810/276106</link>
      <pubDate>Tue, 29 May 2018 00:00:00 -0000</pubDate>
      <description>
        In 1942, Robert Merton wrote that "Incipient and actual attacks upon the integrity of science" meant that science needed to "restate its objectives, seek out its rationale". Some 77 years later we are similarly in an environment where “the people of this country have had enough of experts". It is essential that science is able to withstand rigorous scrutiny to avoid being dismissed, pilloried or ignored. Transparency and reproducibility in the scientific process is a mechanism to meet this challenge and good research data management is a fundamental factor in this.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Replication and Reproducibility in Cross-Cultural Psychology</title>
      <link>http://journals.sagepub.com/doi/abs/10.1177/0022022117744892</link>
      <pubDate>Sat, 26 May 2018 00:00:00 -0000</pubDate>
      <description>
        Replication is the scientific gold standard that enables the confirmation of research findings. Concerns related to publication bias, flexibility in data analysis, and high-profile cases of academic misconduct have led to recent calls for more replication and systematic accumulation of scientific knowledge in psychological science. This renewed emphasis on replication may pose specific challenges to cross-cultural research due to inherent practical difficulties in emulating an original study in other cultural groups. The purpose of the present article is to discuss how the core concepts of this replication debate apply to cross-cultural psychology. Distinct to replications in cross-cultural research are examinations of bias and equivalence in manipulations and procedures, and that targeted research populations may differ in meaningful ways. We identify issues in current psychological research (analytic flexibility, low power) and possible solutions (preregistration, power analysis), and discuss ways to implement best practices in cross-cultural replication attempts.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Transparency on scientific instruments</title>
      <link>http://embor.embopress.org/content/early/2018/05/22/embr.201845853</link>
      <pubDate>Sat, 26 May 2018 00:00:00 -0000</pubDate>
      <description>
        Scientific instruments are at the heart of the scientific process, from 17th‐century telescopes and microscopes, to modern particle colliders and DNA sequencing machines. Nowadays, most scientific instruments in biomedical research come from commercial suppliers [1], [2], and yet, compared to the biopharmaceutical and medical devices industries, little is known about the interactions between scientific instrument makers and academic researchers. Our research suggests that this knowledge gap is a cause for concern.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Before reproducibility must come preproducibility</title>
      <link>https://www.nature.com/articles/d41586-018-05256-0</link>
      <pubDate>Thu, 24 May 2018 00:00:00 -0000</pubDate>
      <description>
        The lack of standard terminology means that we do not clearly distinguish between situations in which there is not enough information to attempt repetition, and those in which attempts do not yield substantially the same outcome. To reduce confusion, I propose an intuitive, unambiguous neologism: ‘preproducibility’. An experiment or analysis is preproducible if it has been described in adequate detail for others to undertake it. Preproducibility is a prerequisite for reproducibility, and the idea makes sense across disciplines.
      </description>

      <category>popular news</category>

    </item>

    <item>
      <title>Facilitating Reproducibility and Collaboration with Literate Programming</title>
      <link>https://hdekk.github.io/escience2018/</link>
      <pubDate>Sun, 13 May 2018 00:00:00 -0000</pubDate>
      <description>
        A fundamental challenge for open science is how best to create and share documents containing computational results. Traditional methods involve maintaining the code, generated tables and figures, and text as separate files and manually assembling them into a finished document. As projects grow in complexity, this approach can lead to procedures which are error prone and hard to replicate. Fortunately, new tools are emerging to address this problem and librarians who provide data services are ideally positioned to provide training. In the workshop we’ll use RStudio to demonstrate how to create a "compilable" document containing all the text elements (including bibliography), as well as the code required to create embedded graphs and tables. We’ll demonstrate how the process facilitates making revisions when, for example, a reviewer has suggested a revision or when there has been a change in the underlying data. We’ll also demonstrate the convenience of integrating version control into the workflow using RStudio’s built-in support for git.
      </description>

      <category>reproducibility talk</category>

    </item>

    <item>
      <title>Evaluating Reproducibility in Computational Biology Research</title>
      <link>https://scholarworks.gvsu.edu/cgi/viewcontent.cgi?article=1682&amp;context=honorsprojects</link>
      <pubDate>Fri, 11 May 2018 00:00:00 -0000</pubDate>
      <description>
        For my Honors Senior Project, I read five research papers in the field of computational biology and attempted to reproduce the results. However, for the most part, this proved a challenge, as many details vital to utilizing relevant software and data had been excluded. Using Geir Kjetil Sandve's paper "Ten Simple Rules for Reproducible Computational Research" as a guide, I discuss how authors of these five papers did and did not obey these rules of reproducibility and how this affected my ability to reproduce their results.
      </description>

      <category>replication study</category>

    </item>

    <item>
      <title>Systematic reviews and evidence synthesis</title>
      <link>https://crln.acrl.org/index.php/crlnews/article/view/16967/18703</link>
      <pubDate>Fri, 11 May 2018 00:00:00 -0000</pubDate>
      <description>
        While comprehensive and expert searching may be part of the traditional aspects of academic librarianship, systematic reviews also require transparency and reproducibility of search methodology. This work is supported by use of reporting guidelines and related librarian expertise. This guide provides resources that are useful to librarians assisting with systematic reviews in a broad range of disciplines outside the biomedical sciences. Because the bulk of published literature on systematic reviews is concentrated in the health sciences, some resources are subject-specific in title, but have broader applications.
      </description>

      <category>reproducibility bibliography</category>

    </item>

    <item>
      <title>Climate Science Can Be More Transparent, Researchers Say</title>
      <link>https://www.scientificamerican.com/article/climate-science-can-be-more-transparent-researchers-say/</link>
      <pubDate>Fri, 11 May 2018 00:00:00 -0000</pubDate>
      <description>
        Top climate scientists say their field can improve its transparency. A group of researchers presented their findings on reproducibility in climate science to the National Academies of Sciences, Engineering and Medicine yesterday as part of a monthslong examination of scientific transparency.
      </description>

      <category>popular news</category>

    </item>

    <item>
      <title>Scientific Research: Reproducibility and Bias in Chemistry</title>
      <link>https://www.decodedscience.org/scientific-research-reproducibility-bias-chemistry/62997</link>
      <pubDate>Fri, 11 May 2018 00:00:00 -0000</pubDate>
      <description>
        When scientists are able to recreate earlier research results, published by other scientists, the research is considered reproducible. But what happens when the results don’t match? It means that the initial research is non-reproducible. Reproducibility, or non-reproducibility, of scientific experiments seems straightforward; it implies that an experimental result is either valid or invalid. In fact, researchers affiliated with Stanford University, Tufts University, and University of Ioannina in Greece concluded in 2005 that a majority of all research findings are false. How do those invalid results end up in scientific papers? A group of Stanford researchers concluded that, in many cases, bias is to blame.
      </description>

      <category>popular news</category>

    </item>

    <item>
      <title>Use Cases of Computational Reproducibility for Scientific Workflows at Exascale</title>
      <link>https://arxiv.org/pdf/1805.00967.pdf</link>
      <pubDate>Sat, 05 May 2018 00:00:00 -0000</pubDate>
      <description>
        We propose an approach for improved reproducibility that includes capturing and relating provenance characteristics and performance metrics, in a hybrid queriable system, the ProvEn server. The system capabilities are illustrated on two use cases: scientific reproducibility of results in the ACME climate simulations and performance reproducibility in molecular dynamics workflows on HPC computing platforms.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>DeepDIVA: A Highly-Functional Python Framework for Reproducible Experiments</title>
      <link>https://arxiv.org/pdf/1805.00329.pdf</link>
      <pubDate>Sat, 05 May 2018 00:00:00 -0000</pubDate>
      <description>
        We introduce DeepDIVA: an infrastructure designed to enable quick and intuitive setup of reproducible experiments with a large range of useful analysis functionality. Reproducing scientific results can be a frustrating experience, not only in document image analysis but in machine learning in general. Using DeepDIVA a researcher can either reproduce a given experiment with a very limited amount of information or share their own experiments with others. Moreover, the framework offers a large range of functions, such as boilerplate code, keeping track of experiments, hyper-parameter optimization, and visualization of data and results. To demonstrate the effectiveness of this framework, this paper presents case studies in the area of handwritten document analysis where researchers benefit from the integrated functionality. DeepDIVA is implemented in Python and uses the deep learning framework PyTorch. It is completely open source, and accessible as Web Service through DIVAServices.
      </description>

      <category>reproducibility infrastructure</category>

    </item>

    <item>
      <title>Open Data &amp; Reproducibility</title>
      <link>https://escholarship.org/uc/item/115856kh</link>
      <pubDate>Thu, 03 May 2018 00:00:00 -0000</pubDate>
      <description>
        Presentation given for Love Data Week 2018.
      </description>

      <category>reproducibility talk</category>

    </item>

    <item>
      <title>Qualitative Coding: Strategies for Transparency and Reproducibility</title>
      <link>https://scholarworks.iu.edu/dspace/bitstream/handle/2022/22052/2018-04-27_wim_meanwell_coding_slides.pdf?sequence=3&amp;isAllowed=y</link>
      <pubDate>Tue, 01 May 2018 00:00:00 -0000</pubDate>
      <description>
        Workshop in methods at IU.
      </description>

      <category>reproducibility talk</category>

    </item>

    <item>
      <title>Open access to data at Yale University</title>
      <link>https://elischolar.library.yale.edu/dayofdata/2017/posters/6/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 -0000</pubDate>
      <description>
        Open access to research data increases knowledge, advances science, and benefits society. Many researchers are now required to share data. Two research centers at Yale have launched projects that support this mission. Both centers have developed technology, policies, and workflows to facilitate open access to data in their respective fields. The Yale University Open Data Access (YODA) Project at the Center for Outcomes Research and Evaluation advocates for the responsible sharing of clinical research data. The Project, which began in 2014, is committed to open science and data transparency, and supports research attempting to produce concrete benefits to patients, the medical community, and society as a whole. Early experience sharing data, made available by Johnson &amp; Johnson (J&amp;J) through the YODA Project, has demonstrated a demand for shared clinical research data as a resource for investigators. To date, the YODA Project has facilitated the sharing of data for over 65 research projects. The Institution for Social and Policy Studies (ISPS) Data Archive is a digital repository that shares and preserves the research produced by scholars affiliated with ISPS. Since its launch in 2011, the Archive holds data and code underlying almost 90 studies. The Archive is committed to the ideals of scientific reproducibility and transparency: It provides free and public access to research materials and accepts content for distribution under a Creative Commons license. The Archive has pioneered a workflow, “curating for reproducibility,” that ensures long term usability and data quality.
      </description>

      <category>reproducibility talk</category>

    </item>

    <item>
      <title>Reflections on the Future of Research Curation and Research Reproducibility</title>
      <link>https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8347157</link>
      <pubDate>Tue, 01 May 2018 00:00:00 -0000</pubDate>
      <description>
        In the years since the launch of the World Wide Web in 1993, there have been profoundly transformative changes to the entire concept of publishing—exceeding all the previous combined technical advances of the centuries following the introduction of movable type in medieval Asia around the year 10001 and the subsequent large-scale commercialization of printing several centuries later by J. Gutenberg (circa 1440). Periodicals in print—from daily newspapers to scholarly journals—are now quickly disappearing, never to return, and while no publishing sector has been unaffected, many scholarly journals are almost unrecognizable in comparison with their counterparts of two decades ago. To say that digital delivery of the written word is fundamentally different is a huge understatement. Online publishing permits inclusion of multimedia and interactive content that add new dimensions to what had been available in print-only renderings. As of this writing, the IEEE portfolio of journal titles comprises 59 online only2 (31%) and 132 that are published in both print and online. The migration from print to online is more stark than these numbers indicate because of the 132 periodicals that are both print and online, the print runs are now quite small and continue to decline. In short, most readers prefer to have their subscriptions fulfilled by digital renderings only.
      </description>

      <category>reproducibility report</category>

    </item>

    <item>
      <title>Dealing with the reproducibility crisis: what can ECRs do about it?</title>
      <link>http://blogs.plos.org/thestudentblog/2018/04/27/dealing-with-the-reproducibility-crisis-what-can-ecrs-do-about-it/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 -0000</pubDate>
      <description>
        Unless you’ve been living under a rock (no judgment, by the way), I’m sure you’ve heard about the reproducibility crisis in scientific research. In 2016, two posts on this blog covered what the main causes of irreproducibility are and what can be done, and how we can reform scientific publishing to value integrity. To briefly recap, a study published in PLOS biology noted that half of preclinical research is not reproducible. The estimated price tag on this irreproducibility is alarming—a whopping $28 billion. In my opinion, however, the most troubling cost of this crisis is its impact on public trust in science.
      </description>

      <category>popular news</category>

    </item>

    <item>
      <title>Computational Reproducibility at Exascale 2017 (CRE2017)</title>
      <link>https://sc17.supercomputing.org/presentation/?id=wksp144&amp;sess=sess132</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 -0000</pubDate>
      <description>
        Reproducibility is an important concern in all areas of computation. As such, computational reproducibility is receiving increasing interest from a variety of parties who are concerned with different aspects of computational reproducibility. Computational reproducibility encompasses several concerns including the sharing of code and data, as well as reproducible numerical results which may depend on operating system, tools, levels of parallelism, and numerical effects. In addition, the publication of reproducible computational results motivates a host of computational reproducibility concerns that arise from the fundamental notion of reproducibility of scientific results that has normally been restricted to experimental science. This workshop combines the Numerical Reproducibility at Exascale Workshops (conducted in 2015 and 2016 at SC) and the panel on Reproducibility held at SC16 (originally a BOF at SC15) to address several different issues in reproducibility that arise when computing at exascale. The workshop will include issues of numerical reproducibility as well as approaches and best practices to sharing and running code.
      </description>

      <category>reproducibility conference</category>

    </item>

    <item>
      <title>Reproducible scientific paper/project</title>
      <link>https://cral.univ-lyon1.fr/labo/perso/mohammad.akhlaghi//pdf/reproducible-paper.pdf</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 -0000</pubDate>
      <description>
        A presentation by Mohammad Akhlaghi from the Centre de Recherche Astrophysique de Lyon (CRAL) on reproducibility through using Make.
      </description>

      <category>reproducibility talk</category>

    </item>

    <item>
      <title>The state of reproducibility in the computational geosciences</title>
      <link>https://eartharxiv.org/kzu8e/</link>
      <pubDate>Mon, 16 Apr 2018 00:00:00 -0000</pubDate>
      <description>
        Figures are essential outputs of computational geoscientific research, e.g. maps and time series showing the results of spatiotemporal analyses. They also play a key role in open reproducible research, where public access is provided to paper, data, and source code to enable reproduction of the reported results. This scientific ideal is rarely practiced as studies, e.g. in biology have shown. In this article, we report on a series of studies to evaluate open reproducible research in the geosciences from the perspectives of authors and readers. First, we asked geoscientists what they understand by open reproducible research and what hinders its realisation. We found there is disagreement amongst authors, and a lack of openness impedes the adoption by authors and readers alike. However, reproducible research also includes the ability to achieve the same results requiring not only accessible but executable source code. Hence, to further examine the reader’s perspective, we searched for open access papers from the geosciences that have code/data attached (in R) and executed the analysis. We encountered several technical issues while executing the code and found differences between the original and reproduced figures. Based on these findings, we propose guidelines for authors to address these.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Estimating the Reproducibility of Experimental Philosophy</title>
      <link>https://psyarxiv.com/sxdah</link>
      <pubDate>Mon, 16 Apr 2018 00:00:00 -0000</pubDate>
      <description>
        For scientific theories grounded in empirical data, replicability is a core principle, for at least two reasons. First, unless we accept to have scientific theories rest on the authority of a small number of researchers, empirical studies should be replicable, in the sense that its methods and procedure should be detailed enough for someone else to conduct the same study. Second, for empirical results to provide a solid foundation for scientific theorizing, they should also be replicable, in the sense that most attempts at replicating the original study that produced them would yield similar results. The XPhi Replicability Project is primarily concerned with replicability in the second sense, that is: the replicability of results. In the past year, several projects have shed doubt on the replicability of key findings in psychology, and most notably social psychology. Because the methods of experimental philosophy have often been close to the ones used in social psychology, it is only natural to wonder to which extent the results experimental philosophers ground their theory are replicable. The aim of the XPhi Replicability Project is precisely to reach a reliable estimate of the replicability of empirical results in experimental philosophy. To this end, several research teams across the world will replicate around 40 studies in experimental philosophy, some among the most cited, others drawn at random. The results of the project will be published in a special issue of the Review of Philosophy and Psychology dedicated to the topic of replicability in cognitive science.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Developer Interaction Traces backed by IDE Screen Recordings from Think aloud Sessions</title>
      <link>http://swat.polymtl.ca/~foutsekh/docs/MSR-Aiko-Trace.pdf</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 -0000</pubDate>
      <description>
        There are two well-known difficulties to test and interpret methodologies for mining developer interaction traces: first, the lack of enough large datasets needed by mining or machine learning approaches to provide reliable results; and second, the lack of "ground truth" or empirical evidence that can be used to triangulate the results, or to verify their accuracy and correctness. Moreover, relying solely on interaction traces limits our ability to take into account contextual factors that can affect the applicability of mining techniques in other contexts, as well hinders our ability to fully understand the mechanics behind observed phenomena. The data presented in this paper attempts to alleviate these challenges by providing 600+ hours of developer interaction traces, from which 26+ hours are backed with video recordings of the IDE screen and developer’s comments. This data set is relevant to researchers interested in investigating program comprehension, and those who are developing techniques for interaction traces analysis and mining.
      </description>

      <category>reproducible paper</category>

      <category>reproducibility infrastructure</category>

    </item>

    <item>
      <title>Reproducibility does not imply, innovation speeds up, and epistemic diversity optimizes discovery of truth in a model-centric meta-scientific framework</title>
      <link>https://arxiv.org/pdf/1803.10118.pdf</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 -0000</pubDate>
      <description>
        Theoretical work on reproducibility of scientific claims has hitherto focused on hypothesis testing as the desired mode of statistical inference.  Focusing on hypothesis testing, however, poses a challenge to identify salient properties of the scientific process related to reproducibility, especially for fields that progress by building, comparing, selecting, and re-building models.  We build a model-centric meta-scientific framework in which scientific discovery progresses by confirming models proposed in idealized experiments.  In a temporal stochastic process of scientific discovery, we define scientists with diverse research strategies who search the true model generating the data.  When there is no replication in the system, the structure of scientific discovery is a particularly simple Markov chain.  We analyze the effect of diversity of research strategies in the scientific community and the complexity of the true model on the time spent at each model, the mean first time to hit the true model and staying with the true model, and the rate of reproducibility given a true model.  Inclusion of replication in the system   breaks the Markov property and fundamentally alters the structure of scientific discovery.  In this case, we analyze aforementioned properties of scientific discovery by an agent-based model.  In our system, the seeming paradox of scientific progress despite irreproducibility persists even in the absence of questionable research practices and incentive structures, as the rate of reproducibility and scientific discovery of the truth are uncorrelated.  We explain this seeming paradox by a combination of research strategies in the population and the state of truth.  Further, we find that innovation speeds up the discovery of truth by making otherwise inaccessible, possibly true models visible to the scientific population.  We also show that epistemic diversity in the scientific population optimizes across a range of desirable properties of scientific discovery.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>archivist: Boost the reproducibility of your research</title>
      <link>http://smarterpoland.pl/index.php/2017/12/boost-the-reproducibility-of-your-research-with-archivist/?utm_content=buffer7c1ab&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 -0000</pubDate>
      <description>
        The safest solution would be to store copies of every object, ever created during the data analysis. All forks, wrong paths, everything. Along with detailed information which functions with what parameters were used to generate each result. Something like the ultimate TimeMachine or GitHub for R objects. With such detailed information, every analysis would be auditable and replicable. Right now the full tracking of all created objects is not possible without deep changes in the R interpreter. The archivist is the light-weight version of such solution.
      </description>

      <category>reproducibility infrastructure</category>

    </item>

    <item>
      <title>Questionable Research Practices in Ecology and Evolution</title>
      <link>https://osf.io/ajyqg</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 -0000</pubDate>
      <description>
        We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Data availability, reusability, and analytic reproducibility: Evaluating the impact of a mandatory open data policy at the journal Cognition</title>
      <link>https://osf.io/preprints/bitss/39cfb/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 -0000</pubDate>
      <description>
        Access to research data is a critical feature of an efficient, progressive, and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data ("analytic reproducibility"). To investigate, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25% pre-policy to 136/174, 78% post-policy), and data that were in-principle reusable (23/104, 22% pre-policy to 85/136, 62%, post-policy). However, for 35 articles with in-principle reusable data, the analytic reproducibility of target outcomes related to key findings was poor: 11 (31%) cases were reproducible without author assistance, 11 (31%) cases were reproducible only with author assistance, and 13 (37%) cases were not fully reproducible despite author assistance. Importantly, original conclusions did not appear to be seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification, and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>The Scientific Filesystem (SCIF)</title>
      <link>https://academic.oup.com/gigascience/advance-article/doi/10.1093/gigascience/giy023/4931737</link>
      <pubDate>Mon, 19 Mar 2018 00:00:00 -0000</pubDate>
      <description>
        Here we present the Scientific Filesystem (SCIF), an organizational format that supports exposure of executables and metadata for discoverability of scientific applications. The format includes a known filesystem structure, a definition for a set of environment variables describing it, and functions for generation of the variables and interaction with the libraries, metadata, and executables located within. SCIF makes it easy to expose metadata, multiple environments, installation steps, files, and entrypoints to render scientific applications consistent, modular, and discoverable. A SCIF can be installed on a traditional host or in a container technology such as Docker or Singularity. We will start by reviewing the background and rationale for the Scientific Filesystem, followed by an overview of the specification, and the different levels of internal modules (“apps”) that the organizational format affords. Finally, we demonstrate that SCIF is useful by implementing and discussing several use cases that improve user interaction and understanding of scientific applications. SCIF is released along with a client and integration in the Singularity 2.4 software to quickly install and interact with Scientific Filesystems. When used inside of a reproducible container, a Scientific Filesystem is a recipe for reproducibility and introspection of the functions and users that it serves.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>An empirical analysis of journal policy effectiveness for computational reproducibility</title>
      <link>http://www.pnas.org/content/pnas/115/11/2584.full.pdf</link>
      <pubDate>Mon, 19 Mar 2018 00:00:00 -0000</pubDate>
      <description>
        A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (i) requesting data and code from authors and (ii) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal Science after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44% of our sample and were able to reproduce the findings for 26%. We find this policy—author remission of data and code postpublication upon request—an improvement over no policy, but currently insufficient for reproducibility.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Reproducibility in Cancer Biology: The who, where and how of fusobacteria and colon cancer</title>
      <link>https://elifesciences.org/articles/28434</link>
      <pubDate>Mon, 19 Mar 2018 00:00:00 -0000</pubDate>
      <description>
        The association between the bacterium Fusobacterium nucleatum and human colon cancer is more complicated than it first appeared.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>A  Windows-Based Framework for Enhancing Scalability and Reproducibility of Large-scale Research Data</title>
      <link>http://www.asee-se.org/proceedings/ASEE2018/papers2018/100.pdf</link>
      <pubDate>Mon, 19 Mar 2018 00:00:00 -0000</pubDate>
      <description>
        Graduate and undergraduate students involved in research projects that generate or analyze extensive datasets use several software applications for data input and processing subject to guidelines for ensuring data quality and availability. Data management guidelines are based on existing practices of the associated academic or funding institutions and may be automated to  minimize human error and maintenance overhead. This paper presents a framework for automating data management processes, and it details the flow of data from generation/acquisition through processing to the output of final reports. It is designed to adapt to changing requirements and limit overhead costs. The paper also presents a representative case study applying the framework to the finite element characterization of the magnetically coupled linear variable reluctance motor. It utilizes modern widely available scripting tools particularly Windows PowerShell® to automate workflows. This task requires generating motor characteristics for several thousands of operating conditions using finite element analysis.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Utilizing Provenance in Reusable Research Objects</title>
      <link>http://www.mdpi.com/2227-9709/5/1/14/htm</link>
      <pubDate>Tue, 13 Mar 2018 00:00:00 -0000</pubDate>
      <description>
        Science is conducted collaboratively, often requiring the sharing of knowledge about computational experiments. When experiments include only datasets, they can be shared using Uniform Resource Identifiers (URIs) or Digital Object Identifiers (DOIs). An experiment, however, seldom includes only datasets, but more often includes software, its past execution, provenance, and associated documentation. The Research Object has recently emerged as a comprehensive and systematic method for aggregation and identification of diverse elements of computational experiments. While a necessary method, mere aggregation is not sufficient for the sharing of computational experiments. Other users must be able to easily recompute on these shared research objects. Computational provenance is often the key to enable such reuse. In this paper, we show how reusable research objects can utilize provenance to correctly repeat a previous reference execution, to construct a subset of a research object for partial reuse, and to reuse existing contents of a research object for modified reuse. We describe two methods to summarize provenance that aid in understanding the contents and past executions of a research object. The first method obtains a process-view by collapsing low-level system information, and the second method obtains a summary graph by grouping related nodes and edges with the goal to obtain a graph view similar to application workflow. Through detailed experiments, we show the efficacy and efficiency of our algorithms.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Provenance and the Different Flavors of Computational Reproducibility</title>
      <link>http://sites.computer.org/debull/A18mar/A18MAR-CD.pdf#page=17</link>
      <pubDate>Tue, 13 Mar 2018 00:00:00 -0000</pubDate>
      <description>
        While reproducibility has been a requirement in natural sciences for centuries, computational experiments have not followed the same standard. Often, there is insufficient information to reproduce computational results described in publications, and in the recent past, this has led to many retractions. Although scientists are aware of the numerous benefits of reproducibility, the perceived amount of work to make results reproducible is a significant disincentive. Fortunately, much of the information needed to reproduce an experiment can be obtained by systematically capturing its provenance. In this paper, we give an overview of different types of provenance and how they can be used to support reproducibility. We also describe a representative set of provenance tools and approaches that make it easy to create reproducible experiments.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Re-Thinking Reproducibility as a Criterion for Research Quality</title>
      <link>http://philsci-archive.pitt.edu/14352/1/Reproducibility_2018_SL.pdf</link>
      <pubDate>Sun, 11 Feb 2018 00:00:00 -0000</pubDate>
      <description>
        A heated debate surrounds the significance of reproducibility as an  indicator for research quality and reliability, with  many commentators linking a  "crisis of reproducibility" to the rise of fraudulent, careless and unreliable practices of knowledge production. Through the analysis of discourse and practices across research fields, I point out that reproducibility is not only interpreted in different ways, but also serves a variety of epistemic functions depending on the research at hand. Given such variation, I argue that the uncritical pursuit of reproducibility as an overarching epistemic value is misleading and potentially damaging to scientific advancement. Requirements for reproducibility, however they are interpreted, are one of many available means to securere liable research outcomes. Furthermore, there are cases wherethe focus on enhancing reproducibility turns out not to foster high-quality research. Scientific communities and Open Science advocates should learn from inferential reasoning from irreproducible data, and promoteincentives for all researchers to explicitly and publicly discuss (1) their methodological commitments, (2) the ways in which they learn from mistakes and problems in everyday practice, and (3) the strategies they use to choose which research component of any project needs to be preserved in the long term, and how.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>EnosStack: A LAMP-like stack for the experimenter</title>
      <link>https://hal.inria.fr/hal-01689726/document</link>
      <pubDate>Mon, 29 Jan 2018 00:00:00 -0000</pubDate>
      <description>
        Reproducibility and repeatability dramatically increase the value of scientific experiments, but remain two challenging goals for the experimenters. Similar to the LAMP stack that considerably eased the web developers life, in this paper, we advocate the need of an analogous software stack to help the experimenters making reproducible research. We propose the EnosStack, an open source software stack especially designed for reproducible scientific experiments. EnosStack enables to easily describe experimental workflows meant to be re-used, while abstracting the underlying infrastructure running them. Being able to switch experiments from a local to a real testbed deployment greatly lower code development and validation time. We describe the abstractions that have driven its design, before presenting a real experiment we deployed on Grid'5000 to illustrate its usefulness. We also provide all the experiment code, data and results to the community.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Building capacity to encourage research reproducibility and #MakeResearchTrue</title>
      <link>http://jmla.mlanet.org/ojs/jmla/article/view/273/584</link>
      <pubDate>Sun, 14 Jan 2018 00:00:00 -0000</pubDate>
      <description>
        In this case study, the authors present one library’s work to help increase awareness of reproducibility and to build capacity for our institution to improve reproducibility of ongoing and future research.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>The Reproducibility Crisis and Academic Libraries</title>
      <link>http://crl.acrl.org/index.php/crl/article/view/16846/18452</link>
      <pubDate>Mon, 08 Jan 2018 00:00:00 -0000</pubDate>
      <description>
        In recent years, evidence has emerged from disciplines ranging from biology to economics that many scientific studies are not reproducible. This evidence has led to declarations in both the scientific and lay press that science is experiencing a “reproducibility crisis” and that this crisis has significant impacts on both science and society, including misdirected effort, funding, and policy implemented on the basis of irreproducible research. In many cases, academic libraries are the natural organizations to lead efforts to implement recommendations from journals, funders, and societies to improve research reproducibility. In this editorial, we introduce the reproducibility crisis, define reproducibility and replicability, and then discusses how academic libraries can lead institutional support for reproducible research.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Scientific replication in the study of social animals</title>
      <link>https://psyarxiv.com/gsz85/</link>
      <pubDate>Sat, 06 Jan 2018 00:00:00 -0000</pubDate>
      <description>
        This chapter is written to help undergraduate students better understand the role of replication in psychology and how it applies to the study of social behavior. We briefly review various replication initiatives in psychology and the events that preceded our renewed focus on replication. We then discuss challenges in interpreting the low rate of replication in psychology, especially social psychology. Finally, we stress the need for better methods and theories to learn the right lessons when replications fail.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>An Open Solution for Urgent Problems: Increasing Research Quality, Reproducibility, &amp; Diversity</title>
      <link>https://vtechworks.lib.vt.edu/handle/10919/80970</link>
      <pubDate>Mon, 11 Dec 2017 00:00:00 -0000</pubDate>
      <description>
        Jeffrey Spies, Ph.D., is the Co-Founder and Chief Technology Officer of the non-profit Center for Open Science. In this presentation, Dr. Spies discusses motivations, values, and common experiences of researchers and scholars in research and publication processes. Spies explores biases toward confirmatory research to the exclusion of exploratory research, funding and reward incentives that conflict with scholarly values, and, costs of delayed research publication -- as measured in human lives. This critical approach to ethics and values in research and publication begs the questions “Where would we be if this [publishing] system were a little more reproducible, a little more efficient?” and asks for an examination of values as revealed by our practice; are we implying that some lives matter more than others? Spies discusses how open output [open access] and open workflow policies and practices assist scholars in aligning their scholarly practices more closely to their scholarly values. For more information: Center for Open Science: https://cos.io Open badges: https://cos.io/our-services/open-science-badges Open Science Framework: https://cos.io/our-products/open-science-framework PrePrint servers: https://cos.io/our-products/osf-preprints/ Registered Reports: https://cos.io/rr Transparency and Openness Promotion Guidelines: https://cos.io/our-services/top-guidelines
      </description>

      <category>reproducibility talk</category>

    </item>

    <item>
      <title>Utilising Semantic Web Ontologies To publish Experimental Workflows</title>
      <link>https://pdfs.semanticscholar.org/d480/3552b1e460c2acaf3848ad360db186257a61.pdf</link>
      <pubDate>Thu, 16 Nov 2017 00:00:00 -0000</pubDate>
      <description>
        Reproducibility in experiments is necessary to verify claims and to reuse prior work in experiments that advance research. However, the traditional model of publication validates research claims through peer-review without taking reproducibility into account. Workflows encapsulate experiment descriptions and components and are suitable for representing reproducibility. Additionally, they can be published alongside traditional patterns as a form of documentation for the experiment which can be combined with linked open data. For reproducibility utilising published datasets, it is necessary to declare the conditions or restrictions for permissible reuse. In this paper, we take a look at the state of workflow reproducibility through a browser based tool and a corresponding study to identify how workflows might be combined with traditional forms of documentation and publication. We also discuss the licensing aspects for data in workflows and how it can be annotated using linked open data ontologies.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>THE DISMAL SCIENCE REMAINS DISMAL, SAY SCIENTISTS</title>
      <link>https://www.wired.com/story/econ-statbias-study/</link>
      <pubDate>Tue, 14 Nov 2017 00:00:00 -0000</pubDate>
      <description>
        The paper inhales more than 6,700 individual pieces of research, all meta-analyses that themselves encompass 64,076 estimates of economic outcomes. That’s right: It’s a meta-meta-analysis. And in this case, Doucouliagos never meta-analyzed something he didn’t dislike. Of the fields covered in this corpus, half were statistically underpowered—the studies couldn’t show the effect they said they did. And most of the ones that were powerful enough overestimated the size of the effect they purported to show. Economics has a profound effect on policymaking and understanding human behavior. For a science, this is, frankly, dismal.
      </description>

      <category>popular news</category>

    </item>

    <item>
      <title>The reproducibility challenge – what researchers need</title>
      <link>http://septentrio.uit.no/index.php/SCS/article/view/4257</link>
      <pubDate>Tue, 14 Nov 2017 00:00:00 -0000</pubDate>
      <description>
        Within the Open Science discussions, the current call for “reproducibility” comes from the raising awareness that results as presented in research papers are not as easily reproducible as expected, or even contradicted those original results in some reproduction efforts. In this context, transparency and openness are seen as key components to facilitate good scientific practices, as well as scientific discovery. As a result, many funding agencies now require the deposit of research data sets, institutions improve the training on the application of statistical methods, and journals begin to mandate a high level of detail on the methods and materials used. How can researchers be supported and encouraged to provide that level of transparency? An important component is the underlying research data, which is currently often only partly available within the article. At Elsevier we have therefore been working on journal data guidelines which clearly explain to researchers when and how they are expected to make their research data available. Simultaneously, we have also developed the corresponding infrastructure to make it as easy as possible for researchers to share their data in a way that is appropriate in their field. To ensure researchers get credit for the work they do on managing and sharing data, all our journals support data citation in line with the FORCE11 data citation principles – a key step in the direction of ensuring that we address the lack of credits and incentives which emerged from the Open Data analysis (Open Data - the Researcher Perspective https://www.elsevier.com/about/open-science/research-data/open-data-report ) recently carried out by Elsevier together with CWTS. Finally, the presentation will also touch upon a number of initiatives to ensure the reproducibility of software, protocols and methods. With STAR methods, for instance, methods are submitted in a Structured, Transparent, Accessible Reporting format; this approach promotes rigor and robustness, and makes reporting easier for the author and replication easier for the reader.
      </description>

      <category>reproducibility guidelines</category>

    </item>

    <item>
      <title>LHC PARAMETER REPRODUCIBILITY</title>
      <link>https://inspirehep.net/record/1635348/files/1635044_45-52.pdf</link>
      <pubDate>Tue, 14 Nov 2017 00:00:00 -0000</pubDate>
      <description>
        This document reviews the stability of the main LHC operational parameters, namely orbit, tune, coupling and chromaticity. The analysis will be based on the LSA settings, measured parameters and real-time trims. The focus will be set on ramp and high energy reproducibility as they are more diflicult to assess and correct on a daily basis for certain parameters like chromaticity and coupling. The reproducibility of the machine in collision will be analysed in detail, in particular the beam offsets at the IPS since the ever decreasing beam sizes at the IPs make beam steering at the IP more and mode delicate.
      </description>

      <category>reproducible paper</category>

    </item>

  </channel>
</rss>